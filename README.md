# Credit Card Fraud Detection
This repository was made to explain the project of **[Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data)**:

**The comments in the codes are in PT-BR**
## Objectives:
- To identify fraudulent credit card transactions, enabling fraud detection in a highly imbalanced class scenario.

## Main libraries used:
- Pandas, Matplotlib, Sklearn and Seaborn.
## Você pode me encontrar:
&nbsp;<a href="https://www.linkedin.com/in/brunofcb/">
  <img src="https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white">
</a>&nbsp;
## [Step 1: EDA]()
<p>In this step, we conduct an exploratory analysis of the data to understand its characteristics, identify potential challenges, and prepare the dataset for modeling.</p>
<p><b>1. Dataset Characteristics:</b></p>

- The dataset has no missing values or textual columns.

- Due to confidentiality reasons, the original features are not available. Variables V1 to V28 are principal components generated by PCA, while Time (elapsed time since the first transaction) and Amount (transaction amount) are the only features that were not transformed by PCA.

<p><b>2. Zero Transaction Amounts:</b></p>

- Approximately 0.64% of the records have a transaction amount equal to zero. We will not apply specific handling, as no additional information is available to interpret these cases.

<p><b>3. Class Imbalance:</b></p>

- The dataset is highly imbalanced, with 99.83% of transactions being non-fraudulent, complicating the analysis. In this context, recall is the main metric, as it improves the detection rate for fraudulent transactions.

[imagem]

<p><b>4. Outlier Analysis:</b></p>

- After analyzing correlations, we selected 4 variables for a deeper inspection, where we identified outliers. We chose to keep these rows, as removing them would disproportionately eliminate more fraudulent transactions, potentially impacting the model.

- When dealing with a highly imbalanced dataset like this one (99.83% non-fraud), most “normal” data points follow well-defined patterns, while fraudulent cases often stand out due to their unique characteristics (the outliers). Excluding these outliers can, in fact, reduce model performance, especially if they represent a large portion of fraud cases.

- The observation that 72% of these outliers are fraudulent indicates that these variables effectively capture the anomalous behavior. Therefore, I opted to retain these outliers to preserve as much fraud information as possible. 

[imagem]

<p><b>5. Model Selection</b>:</p>

- I selected five classification models for this project: Decision Tree, Logistic Regression, SVC, KNN, and Random Forest. These models were chosen to provide a diverse range of approaches and allow for a thorough comparison.

<p><b>6. Train-Test Split and Class Imbalance Handling:</b></p>

- I split the dataset into training and testing sets to evaluate model performance on unseen data. To address the high class imbalance, I applied Random Undersampling to the training data, reducing the number of non-fraudulent transactions. This step helps the models learn to identify fraud more effectively.

<p><b>7. Scaling with MinMaxScaler:</b></p>

- I applied MinMaxScaler to the Time and Amount variables to standardize their range, bringing them onto a similar scale as the PCA-transformed variables (V1 to V28). This scaling aims to improve model performance, especially for algorithms sensitive to feature scaling.
  
<p><b>8.Precision-Recall Curve Plotting:</b></p>

- After training the models, I generated precision-recall curves for each to visualize their performance in detecting fraud, given the dataset’s imbalance. This plot helps me identify the model with the best balance of precision and recall for detecting fraudulent transactions.

## [Step 2: Hyperparameters]()
<p>The objective now is to take the models and tune the hyperparameters to determine which one performs best using GridSearchCV.</p>
<b>Selected Models:</b>

- Logistic Regression (RL)

- Support Vector Classifier (SVC)
  
- K-Nearest Neighbors (KNN)
  
- Random Forest (RF)
  
<p><b> Logistic Regression (RL)</b></p>

- Hyperparameters: {'C': 0.001, 'solver': 'liblinear'}
  
- Performance: The Logistic Regression model demonstrated an improvement in true positives, indicating its effectiveness in correctly identifying fraudulent cases. However, this adjustment also resulted in an increase in false positives, leading to some legitimate cases being incorrectly flagged as fraudulent.
  
- Decision: Given that the model achieved a high number of true positives (152), it was selected as the preferred option for fraud detection.
<p><b> Support Vector Classifier (SVC)</b></p>

- Hyperparameters: {'C': 1000.0, 'class_weight': None, 'kernel': 'linear'}
  
- Performance:The SVC model succeeded in reducing false positives compared to the RL model. However, it showed a decline in the number of true positives, making it less effective in accurately identifying fraud.
  
<p><b> K-Nearest Neighbors (KNN)</b></p>

- Hyperparameters: {'algorithm': 'auto', 'n_neighbors': 3, 'weights': 'uniform'}
  
- Performance: The KNN model was able to increase the number of true positives, improving its identification of fraudulent cases. However, this came at the cost of a higher rate of false positives compared to the SVC model, leading to more legitimate transactions being misclassified.
  
<p><b> Random Forest (RF)</b></p>

- Hyperparameters: {'criterion': 'gini', 'max_depth': 9, 'n_estimators': 200}
  
- Performance: The Random Forest model effectively reduced the number of false positives when compared to KNN, showcasing its ability to better discriminate between fraudulent and non-fraudulent cases. Nevertheless, it experienced a decrease in true positives compared to the KNN model, which might affect its utility in fraud detection.
  
#### Final Decision
- After evaluating the performance of the models, Logistic Regression was ultimately chosen for fraud detection due to its superior performance in identifying true positives. In scenarios where the primary goal is to minimize false positives—thereby avoiding unnecessary costs associated with contacting legitimate customers—the Random Forest model would be the preferred choice.
  
#### Attempts to Improve the Model with Additional Hyperparameters
<p><b> To explore further enhancements, additional hyperparameters were introduced to the Logistic Regression model: </b>b></p>

- Additional Hyperparameters: penalty and max_iter
  
- Best Hyperparameters:
  
- Performance Analysis: Despite incorporating these additional parameters, no significant improvement in model performance was observed. Therefore, the original hyperparameters were retained for final model selection.

#### New Undersampling Approaches
<p><b>NearMiss</b></p>

- Performance: The NearMiss technique resulted in an increase in false positives without yielding any gains in true positives. This indicates that the method was ineffective for our specific case.
  
<p><b>Cluster Centroids</b></p>
- Performance: The Cluster Centroids approach demonstrated a considerable improvement in reducing false positives when compared to NearMiss. However, it still performed worse than the Random Undersampling (RUS) method, suggesting that further exploration of undersampling techniques may be necessary.

## Step 3: Project Overview:

##### Code Utilization for Cost Estimation
<p>To illustrate the practical application of the model, the following code demonstrates how to calculate the costs associated with fraud detection using the confusion matrix generated from the model's predictions.</p>

[imagem]

##### Explanation of the Code
- Confusion Matrix: The confusion matrix is generated to evaluate the model's performance, allowing the identification of true positives, false positives, true negatives, and false negatives.
  
- Assumed Costs: The code assumes that each fraudulent transaction results in a loss of R$ 1000 and that the cost to analyze a transaction is R$ 0.25. These values can be adjusted according to the realities of each business.
  
- Cost Calculation: The total cost of running the model is calculated by multiplying the number of undetected frauds by the cost of fraud and the number of legitimate transactions that were incorrectly classified as fraud by the cost of analysis.
  
<p> This project not only demonstrates the viability of fraud detection through machine learning but also provides a solid foundation for businesses to develop effective, customized solutions. Implementing the model will enable organizations to operate more safely and efficiently, protecting their assets and improving customer satisfaction. The cost and performance metrics provided help quantify the financial impact of the model, allowing informed decisions about fraud prevention strategies.</p>
